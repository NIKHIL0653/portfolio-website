<h2>Introduction to Reinforcement Learning and the CartPole Problem</h2>
<p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, RL doesn't rely on labeled training data. Instead, the agent receives rewards or penalties based on its actions, learning through trial and error to maximize cumulative rewards.</p>

<p>The CartPole problem is a classic RL benchmark from OpenAI Gym. It involves balancing a pole on a moving cart by applying forces (left or right) to the cart. The environment provides four observations: cart position, cart velocity, pole angle, and pole angular velocity. The goal is to keep the pole upright for as long as possible, with the episode ending if the pole falls beyond a certain angle or the cart moves out of bounds.</p>

<p>This project implements a Deep Q-Network (DQN) agent to solve the CartPole problem, demonstrating key RL concepts including Q-learning, experience replay, and neural network approximation of value functions.</p>

<img src="/images/blog/game.png" alt="CartPole Environment Visualization" style="display: block; margin: 0 auto;">

<h2>Understanding Deep Q-Learning (DQN)</h2>
<p>Traditional Q-learning uses a table to store Q-values for each state-action pair. However, for continuous or high-dimensional state spaces like CartPole, this becomes impractical. Deep Q-Learning addresses this by using a neural network to approximate the Q-function.</p>

<p>The core idea is to train a neural network that takes the current state as input and outputs Q-values for each possible action. The agent selects actions using an epsilon-greedy policy: with probability ε, it explores by choosing a random action, and with probability 1-ε, it exploits by choosing the action with the highest Q-value.</p>

<p>Key components of our DQN implementation include:</p>
<ul>
    <li><strong>Neural Network Architecture</strong>: A simple feedforward network with two hidden layers (128 and 64 neurons) using ReLU activation</li>
    <li><strong>Target Network</strong>: A separate network for computing target Q-values, updated periodically to improve stability</li>
    <li><strong>Experience Replay</strong>: A buffer storing past experiences to break correlation between consecutive samples</li>
    <li><strong>Double DQN</strong>: Uses the policy network for action selection and target network for evaluation to reduce overestimation</li>
</ul>

<h2>Reward Shaping and Training Stability</h2>
<p>Effective reward shaping is crucial for stable learning. Our implementation includes:</p>
<ul>
    <li><strong>Survival Bonus</strong>: Additional rewards for keeping the pole upright and centering the cart</li>
    <li><strong>Failure Penalty</strong>: Strong negative reward (-100) when the pole falls</li>
    <li><strong>Velocity Penalties</strong>: Small penalties for high velocities to encourage smooth control</li>
</ul>

<p>Training stability is enhanced through:</p>
<ul>
    <li><strong>Gradient Clipping</strong>: Prevents exploding gradients by limiting the norm of gradients</li>
    <li><strong>Q-Value Clipping</strong>: Constrains Q-values between -10 and 10 to prevent instability</li>
    <li><strong>Target Network Updates</strong>: Less frequent updates (every 2000 steps) for better convergence</li>
</ul>

<h2>Tech Stack and Implementation Details</h2>
<p>The project is built using Python with several key libraries:</p>
<ul>
    <li><strong>PyTorch</strong>: Deep learning framework for implementing the neural networks and optimization</li>
    <li><strong>OpenAI Gym</strong>: Provides the CartPole environment and standardized RL interfaces</li>
    <li><strong>Pygame</strong>: Creates an interactive training interface with real-time visualization</li>
    <li><strong>NumPy</strong>: Handles numerical computations and data manipulation</li>
</ul>

<p>The codebase is organized into several modules:</p>
<ul>
    <li><code>agent.py</code>: Contains the DQNAgent class with training logic, action selection, and reward shaping</li>
    <li><code>model.py</code>: Defines the DQN neural network architecture</li>
    <li><code>replay_buffer.py</code>: Implements the experience replay mechanism</li>
    <li><code>train.py</code>: Command-line training script with configurable hyperparameters</li>
    <li><code>console_game.py</code>: Interactive Pygame-based training interface</li>
    <li><code>play.py</code>: Inference script for running trained models</li>
</ul>

<h2>Interactive Training Interface</h2>
<p>One of the standout features is the interactive training interface built with Pygame. This provides:</p>
<ul>
    <li><strong>Real-time Visualization</strong>: Live rendering of the cart-pole system</li>
    <li><strong>Performance Metrics</strong>: Displays current score, target score, and statistics panel</li>
    <li><strong>Progress Tracking</strong>: Shows recent episode scores and performance history chart</li>
    <li><strong>Automatic Model Saving</strong>: Saves the model when achieving the target score (>150)</li>
</ul>

<p>The interface uses a clean layout with the game area on the left and statistics panel on the right, making it easy to monitor training progress.</p>

<img src="/images/blog/game-win.png" alt="CartPole Game Win" style="display: block; margin: 0 auto;">

<h2>Training Results and Performance</h2>
<p>The agent is trained using the following key hyperparameters:</p>
<ul>
    <li>Learning Rate: 5e-4 (agent.py) / 1e-3 (train.py)</li>
    <li>Discount Factor (γ): 0.995</li>
    <li>Batch Size: 32-64</li>
    <li>Replay Buffer Capacity: 10,000-30,000</li>
    <li>Epsilon Decay: From 1.0 to 0.02 over 10,000-20,000 steps</li>
</ul>

<p>The project achieves the CartPole solving criteria:</p>
<ul>
    <li><strong>Gym Standard</strong>: Average score > 195 over 100 consecutive episodes</li>
    <li><strong>Interactive Target</strong>: Score > 150 in a single episode</li>
</ul>

<p>Training typically converges within 500-1000 episodes, depending on the hyperparameters and random seed.</p>

<h2>Key Techniques and Best Practices</h2>
<p>Several advanced techniques are implemented to improve learning:</p>
<ul>
    <li><strong>Experience Replay</strong>: Random sampling from past experiences reduces correlation and improves sample efficiency</li>
    <li><strong>Double DQN</strong>: Mitigates Q-value overestimation by separating action selection and evaluation</li>
    <li><strong>Target Network</strong>: Provides stable targets for Q-value updates</li>
    <li><strong>Epsilon-Greedy Exploration</strong>: Balances exploration and exploitation with decaying epsilon</li>
    <li><strong>Reward Shaping</strong>: Provides denser reward signals to guide learning</li>
</ul>

<h2>Model Persistence and Inference</h2>
<p>The project includes comprehensive model saving and loading functionality:</p>
<ul>
    <li><strong>Checkpoint Saving</strong>: Saves model weights, optimizer state, and training progress</li>
    <li><strong>Inference Mode</strong>: Greedy policy execution for evaluation</li>
    <li><strong>Video Recording</strong>: Optional video capture of agent performance</li>
</ul>

<p>This allows for easy deployment and evaluation of trained models.</p>

<h2>Conclusion and Future Improvements</h2>
<p>This CartPole agent project demonstrates the power of Deep Q-Learning in solving continuous control problems. The implementation includes modern RL techniques and provides both command-line and interactive training options.</p>

<p>Potential improvements could include:</p>
<ul>
    <li><strong>Prioritized Experience Replay</strong>: Focus on important experiences</li>
    <li><strong>Multi-step Learning</strong>: Consider n-step returns for better credit assignment</li>
    <li><strong>Distributional RL</strong>: Learn value distributions instead of expected values</li>
    <li><strong>Advanced Architectures</strong>: Incorporate attention mechanisms or convolutional layers</li>
</ul>

<p>The project serves as an excellent starting point for understanding and implementing reinforcement learning algorithms, with clear code organization and comprehensive documentation.</p>

<h2>Explore the Codebase</h2>
<p>Check out the complete source code, documentation, and setup instructions on <a href="https://github.com/NIKHIL0653/cart-pole_agent-RL" target="Cart-Pole-Agent">GitHub.</a></p>

</body>
</html>